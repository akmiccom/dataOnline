# ============================
# main.py
# ============================
from config import CSV_PATH, DB_PATH, ARCHIVE_PATH, LOG_PATH
from config import QUERY, SPREADSHEET_IDS
import scraper
from csv_to_database import csv_to_database
from databese_to_gspread import search_hall_and_load_data, preprocess_result_df
from databese_to_gspread import get_medals_summary, medals_summary_to_gspread
from databese_to_gspread import extract_and_merge_model_data, extract_merge_all_model_date
from databese_to_gspread import merge_all_model_date_to_gspread
from utils import upgrade_uc_if_needed, log_banner
from utils import connect_to_spreadsheet, get_existing_worksheet
from logger_setup import setup_logger
logger = setup_logger("main", log_file=LOG_PATH)


# ============================

PREF, HALL_NAME = "æ±äº¬éƒ½", "EXA FIRST"
DAYS_AGO = 1
PERIOD = 1

# PREF, HALL_NAME = "æ±äº¬éƒ½", "ã‚³ãƒ³ã‚µãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ã‚¨ãƒ•æˆå¢—"
# DAYS_AGO = 1
# PERIOD = 1

# PREF, HALL_NAME = "åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶å‚æˆ¸1000"
# DAYS_AGO = 1
# PERIOD = 1

# PREF, HALL_NAME = "åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶ç‹­å±±åº—"
# DAYS_AGO = 1
# PERIOD = 1

# PREF, HALL_NAME = "åŸ¼ç‰çœŒ", "ãƒ‘ãƒ¼ãƒ«ã‚·ãƒ§ãƒƒãƒ—ã¨ã‚‚ãˆå·è¶Šåº—"
# PREF, HALL_NAME = "åŸ¼ç‰çœŒ", "ãƒ‘ãƒ©ãƒƒãƒ„ã‚©å·è¶Šåº—"

SCRAPER = True
TO_DATABESE = True
TO_SPREADSHEET = True

# ============================


MODEL_LIST = [
    "ãƒã‚¤ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼V",
    "ã‚´ãƒ¼ã‚´ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼3",
    "ã‚¢ã‚¤ãƒ ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼EX-TP",
    "ãƒ•ã‚¡ãƒ³ã‚­ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼2",
    "ãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ã‚¦ãƒ«ãƒˆãƒ©ãƒŸãƒ©ã‚¯ãƒ«ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
    "ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼ã‚¬ãƒ¼ãƒ«ã‚º",
    "ãƒãƒƒãƒ”ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼VIII",
]

SHEET_NAME_RANK = "RANKING"
SHEET_NAME_COMPARE = "HISTORY"

AREA_MAP_PATH = f"C:/python/dataOnline/anaslo_02/json/{HALL_NAME}_area_map.json"

log_banner(f"ğŸ“Š {HALL_NAME} ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")

if SCRAPER:
    URL = f"https://ana-slo.com/ãƒ›ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿/{PREF}/{HALL_NAME}-ãƒ‡ãƒ¼ã‚¿ä¸€è¦§/"
    upgrade_uc_if_needed()
    driver = scraper.start_google_chrome("https://www.google.com/")
    for days_ago in range(DAYS_AGO, DAYS_AGO + PERIOD):
        scraper.scraper_for_data(
            driver, days_ago, scraper.REMOVE_ADS_SCRIPT, CSV_PATH, PREF, URL
        )

if TO_DATABESE:
    csv_to_database(DB_PATH, CSV_PATH, ARCHIVE_PATH)

if TO_SPREADSHEET:    
    spreadsheet = connect_to_spreadsheet(SPREADSHEET_IDS[HALL_NAME])
    ws = get_existing_worksheet(spreadsheet, SHEET_NAME_RANK)
    if ws is None:
        print("ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        exit()
    
    df_from_db = search_hall_and_load_data(HALL_NAME, QUERY)
    df = preprocess_result_df(df_from_db, AREA_MAP_PATH)
    medals_summary_to_gspread(
        df, MODEL_LIST, spreadsheet, get_medals_summary, sheet_name=SHEET_NAME_RANK
    )
    merged_by_model = extract_merge_all_model_date(
        extract_and_merge_model_data, df, MODEL_LIST
    )
    merge_all_model_date_to_gspread(
        merged_by_model, spreadsheet, sheet_name=SHEET_NAME_COMPARE
    )

logger.info(f"ğŸ‰ {HALL_NAME} ãƒ‡ãƒ¼ã‚¿åé›†çµ‚äº†")
logger.info("=" * 40)
