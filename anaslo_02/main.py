# ============================
# main.py
# ============================
from config import CSV_PATH, DB_PATH, ARCHIVE_PATH, LOG_PATH
from config import QUERY, SPREADSHEET_IDS, MODEL_LIST
from config import today
import scraper
from csv_to_database import csv_to_database
from databese_to_gspread import search_hall_and_load_data, preprocess_result_df
from databese_to_gspread import merge_history_by_model, history_data_by_model
from databese_to_gspread import medal_rate_by_unit, medal_rate_by_island
from databese_to_gspread import medal_rate_by_model, medal_rate_by_day
from databese_to_gspread import dataFrame_to_gspread
from utils import upgrade_uc_if_needed, connect_to_spreadsheet, log_banner
from logger_setup import setup_logger

logger = setup_logger("main", log_file=LOG_PATH)


# ============================
# å…¨ãƒ›ãƒ¼ãƒ«å®šç¾©ï¼šéƒ½é“åºœçœŒ + ãƒ›ãƒ¼ãƒ«å + æ—¥æ•°ç¯„å›²
# ============================

HALL_LIST = [
    # ("æ±äº¬éƒ½", "EXA FIRST", 1, 1),
    # ("æ±äº¬éƒ½", "ã‚³ãƒ³ã‚µãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ã‚¨ãƒ•æˆå¢—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶å‚æˆ¸1000", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶ç‹­å±±åº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ã¿ãšã»å°uno", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ã‚ªãƒ¼ã‚¿å¿—æœ¨é§…å‰åº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ã‚°ãƒ©ãƒ³ãƒ‰ã‚ªãƒ¼ã‚¿æ–°åº§é§…å‰åº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶å‚æˆ¸ã«ã£ã•ã„åº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "tohoå·è¶Šåº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶ã¿ãšã»å°åº—", 1, 1),
    ("åŸ¼ç‰çœŒ", "ã‚°ãƒ©ãƒ³ãƒ‰ã‚ªãƒ¼ã‚¿æ–°åº§é§…å‰åº—", 200, 300),
    # ("åŸ¼ç‰çœŒ", "ãƒ‹ãƒ¥ãƒ¼ã‚¯ãƒ©ã‚¦ãƒ³å·è¶Š2å·åº—", 100, 100),
    # ("åŸ¼ç‰çœŒ", "ãƒ‘ãƒ¼ãƒ«ã‚·ãƒ§ãƒƒãƒ—ã¨ã‚‚ãˆå·è¶Šåº—", 1, 1),
    # ("åŸ¼ç‰çœŒ", "ãƒ‘ãƒ©ãƒƒãƒ„ã‚©å·è¶Šåº—", 1, 1),
]

SCRAPER = True
TO_DATABESE = True
TO_SPREADSHEET = False

# ============================


if SCRAPER:
    upgrade_uc_if_needed()
    driver = scraper.start_google_chrome("https://www.google.com/")

for PREF, HALL_NAME, DAYS_AGO, PERIOD in HALL_LIST:
    log_banner(f"ğŸ“Š {HALL_NAME} ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    if SCRAPER:
        URL = f"https://ana-slo.com/ãƒ›ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿/{PREF}/{HALL_NAME}-ãƒ‡ãƒ¼ã‚¿ä¸€è¦§/"
        for days_ago in range(DAYS_AGO, DAYS_AGO + PERIOD):
            scraper.scraper_for_data(
                driver, days_ago, scraper.REMOVE_ADS_SCRIPT, CSV_PATH, PREF, URL
            )

    if TO_DATABESE:
        csv_to_database(DB_PATH, CSV_PATH, ARCHIVE_PATH)

    if TO_SPREADSHEET:    
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã¨å‰å‡¦ç†
        AREA_MAP_PATH = f"C:/python/dataOnline/anaslo_02/json/{HALL_NAME}_area_map.json"
        df_from_db = search_hall_and_load_data(HALL_NAME, QUERY)
        df = preprocess_result_df(df_from_db, AREA_MAP_PATH)
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ¥ç¶š
        spreadsheet = connect_to_spreadsheet(SPREADSHEET_IDS[HALL_NAME])
        
        # MODEL_RATE ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
        model_rate = medal_rate_by_model(df)
        dataFrame_to_gspread(model_rate, spreadsheet, sheet_name="MODEL_RATE")

        # ISLAND_RATE ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
        island_rate = medal_rate_by_island(df)
        dataFrame_to_gspread(island_rate, spreadsheet, sheet_name="ISLAND_RATE")

        # UNIT_RATE ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
        unit_rate = medal_rate_by_unit(df)
        dataFrame_to_gspread(unit_rate, spreadsheet, sheet_name="UNIT_RATE")
        
        # HISTORY ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
        history = merge_history_by_model(history_data_by_model, df, MODEL_LIST)
        dataFrame_to_gspread(history, spreadsheet, sheet_name="HISTORY")

        # DAY_RATE ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
        for day_target in range(today.day - 1, today.day + 3):
            marged_day = medal_rate_by_day(df, day_target)
            dataFrame_to_gspread(marged_day, spreadsheet, sheet_name=f"DAY{day_target}")
    
    logger.info(f"ğŸ‰ {HALL_NAME} ãƒ‡ãƒ¼ã‚¿åé›†çµ‚äº†")
    logger.info("=" * 40)

if SCRAPER:
    driver.close()