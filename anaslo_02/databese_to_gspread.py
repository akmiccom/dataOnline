# ============================
# detabase_to_gspread.py
# ============================
from gspread_dataframe import get_as_dataframe, set_with_dataframe
import gspread
import pandas as pd
import numpy as np
import json
import sqlite3
import datetime
from dateutil.relativedelta import relativedelta
from utils import connect_to_spreadsheet
from logger_setup import setup_logger
from config import LOG_PATH, DB_PATH, JSONF, SPREADSHEET_IDS


logger = setup_logger("datebase_to_gspread", log_file=LOG_PATH)


def search_hall_and_load_data(search_word, query):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    logger.info(f"ğŸš€ DBæ¥ç¶š: {DB_PATH}")
    cursor.execute(
        "SELECT hall_id, name FROM halls WHERE name LIKE ?", ("%" + search_word + "%",)
    )
    results = cursor.fetchall()
    # çµæœè¡¨ç¤º
    if results:
        logger.info(f"ğŸ” '{search_word}' ã‚’å«ã‚€ãƒ›ãƒ¼ãƒ«åã®æ¤œç´¢çµæœ:")
        for hall_id, hall_name in results:
            logger.info(f" - hall_name: {hall_name}, hall_id: {hall_id}")
    else:
        logger.error(f"âŒ '{search_word}' ã‚’å«ã‚€ãƒ›ãƒ¼ãƒ«åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    df_from_db = pd.read_sql_query(query, conn, params=(hall_name,))
    conn.close()
    logger.info(f"ğŸ›‘ DBæ¥ç¶šçµ‚äº†: {DB_PATH}")

    return df_from_db


def grape_calculator_myfive(game, bb, rb, medals, cherry=True):
    bb_medals = 239.25
    rb_medals = 95.25
    replay_rate = 0.411
    if cherry:
        cherry_rate_high = 0.04228
    else:
        cherry_rate_high = 0.05847
    denominator_inner = (
        -medals
        - (
            game * 3
            - (
                bb * bb_medals
                + rb * rb_medals
                + game * replay_rate
                + game * cherry_rate_high
            )
        )
    ) / 8
    grape_rate = (game / denominator_inner) - ((game / denominator_inner) * 2)

    return grape_rate


def assign_area(unit_no, json_file_path):
    with open(json_file_path, "r", encoding="utf-8") as f:
        area_map = json.load(f)
    for rule in area_map:
        if rule["start"] <= unit_no <= rule["end"]:
            return rule["area"]
    return "ãã®ä»–"


def preprocess_result_df(df, json_path):
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    df["date"] = pd.to_datetime(df["date"])
    df.drop(columns=["result_id", "hall_id", "model_id"], inplace=True)
    df = df[
        ["hall_name", "date", "model_name", "unit_no", "game", "BB", "RB", "medals"]
    ]
    df["BB"] = df["BB"].replace(0, np.nan)
    df["RB"] = df["RB"].replace(0, np.nan)
    df["BB_rate"] = (df["game"] / df["BB"]).round(1)
    df["RB_rate"] = (df["game"] / df["RB"]).round(1)
    df["Total_rate"] = (df["game"] / (df["BB"] + df["RB"])).round(1)
    df["Grape_rate"] = grape_calculator_myfive(
        df["game"], df["BB"], df["RB"], df["medals"], cherry=True
    ).round(2)
    df["day"] = df["date"].dt.day
    df["month"] = df["date"].dt.month
    df["weekday"] = df["date"].dt.weekday
    df["unit_last"] = df["unit_no"].astype(str).str[-1]
    df["area"] = df["unit_no"].apply(lambda x: assign_area(x, json_path))

    logger.info(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†: {df.shape[0]} rows")
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape[0]} x {df.shape[1]}")
    model_list = df["model_name"].unique()
    logger.debug(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
    for i, model in enumerate(model_list):
        logger.debug(f"{i}: {model}")

    return df


def get_medals_summary(df, start_date, end_date, model_name):
    # 7æ—¥é–“ã®ãƒ¡ãƒ€ãƒ«é›†è¨ˆ
    df_tmp = df[
        (df["model_name"] == model_name)
        & (df["date"].dt.date <= start_date)
        & (df["date"].dt.date >= end_date)
    ].copy()

    medals = df_tmp.pivot_table(
        index=["model_name", "area", "unit_no"],
        columns="date",
        values="medals",
        aggfunc="sum",
        margins=True,
        margins_name="Total",
    )
    medals.drop(labels="Total", level=0, inplace=True)
    medals["Rank"] = medals["Total"].rank(method="min", ascending=True).astype(int)
    medals.columns = pd.MultiIndex.from_product([["MEDALS"], medals.columns])

    logger.info(f"ğŸ¯ ãƒ¡ãƒ€ãƒ«é›†è¨ˆå®Œäº†: {model_name}")

    return medals


def medals_summary_to_gspread(
    df, model_list, spreadsheet, get_medals_summary, sheet_name
):
    logger.info(f"ğŸ“† æœ¬æ—¥ã‚ˆã‚Š7æ—¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™: {sheet_name}")

    today = datetime.date.today()
    start_date = today + datetime.timedelta(days=-1)
    end_date = today + datetime.timedelta(days=-7)
    logger.info(f"   ğŸ“ åŸºæº–æ—¥: {start_date}, çµ‚äº†æ—¥: {end_date}")

    sheet = spreadsheet.worksheet(sheet_name)
    sheet.clear()
    next_row = 1
    for model in model_list:
        medals = get_medals_summary(df, start_date, end_date, model)
        set_with_dataframe(sheet, medals, row=next_row, include_index=True)
        existing = get_as_dataframe(sheet, evaluate_formulas=True)
        next_row += medals.shape[0] + 5
        logger.info(f"   âœ… è¿½åŠ å®Œäº†: {model}")
    sheet.update_cell(1, 1, today.strftime("%Y-%m-%d: UPDATED"))
    logger.info(f"ğŸ’¾ ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†: {sheet_name}")


def extract_and_merge_model_data(df, model_name, period_month=1):
    start_date = datetime.date.today()
    end_date = start_date - relativedelta(months=period_month, days=start_date.day + 15)

    # å¯¾è±¡æœŸé–“ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    logger.info(f"ğŸ” ãƒ¢ãƒ‡ãƒ«: {model_name} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­...")
    df_filtered = df.copy()
    df_filtered = df_filtered[
        (df_filtered["date"].dt.date <= start_date)
        & (df_filtered["date"].dt.date >= end_date)
    ]
    df_filtered = df_filtered[(df_filtered["model_name"] == model_name)]
    if df_filtered.empty:
        logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«: {model_name} ã®æœŸé–“ä¸­ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return pd.DataFrame()

    # å„ç¨®ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
    pivot_targets = ["medals", "game", "RB_rate", "Total_rate", "Grape_rate"]
    index_targets = ["area", "model_name", "unit_no"]
    columns_targets = ["date"]
    pivot_results = {}
    for col in pivot_targets:
        table = df_filtered.pivot_table(
            index=index_targets,
            columns=columns_targets,
            values=col,
            aggfunc="sum",
        )
        # æ—¥ä»˜åˆ—ã‚’åè»¢ãƒ»ã‚¹ãƒ©ã‚¤ã‚¹
        pivot_results[col] = table.iloc[:, 7:].iloc[:, ::-1]

    medals = pivot_results["medals"]
    game = pivot_results["game"]
    rb_rate = pivot_results["RB_rate"]
    total_rate = pivot_results["Total_rate"]
    grape_rate = pivot_results["Grape_rate"]
    medal_rate = ((medals + game * 3) / (game * 3)).round(3)
    # ãƒ¡ãƒ€ãƒ«ç´¯ç©3æ—¥é–“ãƒ»7æ—¥é–“
    rolling7 = (
        medals.iloc[:, ::-1].rolling(7, min_periods=7, axis=1).sum().iloc[:, ::-1]
    )
    rolling5 = (
        medals.iloc[:, ::-1].rolling(5, min_periods=3, axis=1).sum().iloc[:, ::-1]
    )
    rolling3 = (
        medals.iloc[:, ::-1].rolling(3, min_periods=3, axis=1).sum().iloc[:, ::-1]
    )
    # 7æ—¥é–“ç´¯ç©ã¨ãƒ©ãƒ³ã‚¯
    medal_rank = (
        rolling7.rank(method="min", ascending=True)
        .fillna(0)
        .replace([np.inf, -np.inf], 0)
        .astype(int)
    )

    # MultiIndexåŒ–ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ã‘ï¼‰
    labeled_tables = [
        ("RANK", medal_rank),
        ("7ROLLING", rolling7),
        ("5ROLLING", rolling5),
        ("3ROLLING", rolling3),
        ("MEDALS", medals),
        ("RATE_MEDAL", medal_rate),
        ("GAME", game),
        ("RB_RATE", rb_rate),
        ("TOTAL_RATE", total_rate),
        # ("GRAPE_RATE", grape_rate),
    ]

    # ãƒ©ãƒ™ãƒ«ã‚’ MultiIndex ã«ä»˜ã‘ã‚‹
    for label, df_table in labeled_tables:
        df_table.columns = pd.MultiIndex.from_product([[label], df_table.columns])

    # åˆ—ã‚’äº¤äº’ã«æ•´åˆ—ã—ã¦çµ±åˆãƒ»NaNé™¤å»
    interleaved_cols = [
        col
        for col_group in zip(*(df.columns for _, df in labeled_tables))
        for col in col_group
    ]
    merged = pd.concat([df for _, df in labeled_tables], axis=1)[interleaved_cols]
    merged = merged[~merged.iloc[:, 1].isna()]  # å‰æ—¥ãŒNaNã®è¡Œã¯å‰Šé™¤

    # ã‚¨ãƒªã‚¢ã”ã¨ã«ç©ºè¡ŒæŒ¿å…¥ã—ã¦æ•´å½¢
    merged_by_area = pd.DataFrame()
    for area in merged.index.get_level_values("area").unique():
        area_merged = merged.xs(area, level="area", drop_level=False)
        if not area_merged.empty:
            empty_index = pd.MultiIndex.from_tuples(
                [("", " ", " ")], names=merged.index.names
            )
            empty_row = pd.DataFrame(
                [[""] * area_merged.shape[1]],
                index=empty_index,
                columns=area_merged.columns,
            )
            merged_by_area = pd.concat([merged_by_area, area_merged, empty_row])

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤
    merged_by_area = merged_by_area.droplevel("area")
    logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«: {model_name} ã®å‡¦ç†å®Œäº†ï¼ˆè¡Œæ•°: {len(merged_by_area)}ï¼‰")

    return merged_by_area


def extract_merge_all_model_date(process_func, df, model_list):

    merged_by_model = pd.DataFrame()
    for model_name in model_list:
        merged_by_area = extract_and_merge_model_data(df, model_name)
        # ãƒ¢ãƒ‡ãƒ«é–“ã®åŒºåˆ‡ã‚Šç”¨ç©ºè¡Œè¿½åŠ ã—ã¦çµåˆ
        if not merged_by_area.empty:
            empty_index = pd.MultiIndex.from_tuples(
                [(" ", " ")], names=merged_by_area.index.names
            )
            empty_row = pd.DataFrame(
                [[""] * merged_by_area.shape[1]],
                index=empty_index,
                columns=merged_by_area.columns,
            )
            merged_by_model = pd.concat(
                [merged_by_model, merged_by_area, empty_row], axis=0
            )

    return merged_by_model


def create_pivot_table(
    df,
    start_date,
    end_date,
    pivot_targets,
    index_targets,
    columns_targets,
    csv_path,
    hall_name,
):
    df_filtered = df.copy()
    df_filtered = df_filtered[
        (df_filtered["date"].dt.date <= start_date)
        & (df_filtered["date"].dt.date >= end_date)
    ]

    pivot_results = {}
    for col in pivot_targets:
        table = df_filtered.pivot_table(
            index=index_targets,
            columns=columns_targets,
            values=col,
            aggfunc="sum",
            margins=True,
            margins_name="total",
        )
        pivot_results[col] = table
    # pivot_results[col] = table.iloc[:, ::-1]

    game = pivot_results["game"]
    medals = pivot_results["medals"]
    rb = pivot_results["RB"]
    bb = pivot_results["BB"]
    rb_rate = (game / rb).round(1)
    total_rate = (game / (bb + rb)).round(1)
    medal_rate = ((medals + game * 3) / (game * 3)).round(3)

    labeled_tables = [
        ("GAME", game),
        ("MEDALS", medals),
        ("RB_RATE", rb_rate),
        ("TOTAL_RATE", total_rate),
        ("MEDAL_RATE", medal_rate),
        ("BB", bb),
        ("RB", rb),
    ]

    # ãƒ©ãƒ™ãƒ«ã‚’ MultiIndex ã«ä»˜ã‘ã‚‹
    for label, df_table in labeled_tables:
        df_table.columns = pd.MultiIndex.from_product([[label], df_table.columns])

    # åˆ—ã‚’äº¤äº’ã«æ•´åˆ—ã—ã¦çµ±åˆãƒ»NaNé™¤å»
    interleaved_cols = [
        col
        for pair in zip(
            game.columns,
            medals.columns,
            bb.columns,
            rb.columns,
            medal_rate.columns,
            rb_rate.columns,
            total_rate.columns,
        )
        for col in pair
    ]

    merged = pd.concat([game, medals, medal_rate, bb, rb, rb_rate, total_rate], axis=1)[
        interleaved_cols
    ]
    merged.to_csv(csv_path)

    return medal_rate


def medal_rate_summary_to_gspread(df, hall_name, spreadsheet):
    today = datetime.date.today()
    start_date = today
    end_date = start_date - relativedelta(months=6, days=start_date.day - 1)
    logger.info(f"ğŸ“† å¯¾è±¡æœŸé–“: {end_date} ã€œ {start_date}")
    logger.info(f"ğŸ¢ å¯¾è±¡ãƒ›ãƒ¼ãƒ«: {hall_name}")

    csv_path = f"{hall_name}_medal_rate.csv"
    pivot_targets = ["game", "medals", "BB", "RB"]
    index_targets = ["area", "unit_no"]
    columns_targets = ["day"]

    medal_rate = create_pivot_table(
        df,
        start_date,
        end_date,
        pivot_targets,
        index_targets,
        columns_targets,
        csv_path,
        hall_name,
    )
    medal_rate.to_csv(csv_path)

    target_rate = 1.05
    medal_rate[("MEDAL_RATE", f"count_{target_rate}+")] = (
        medal_rate.iloc[:, :-1] >= target_rate
    ).sum(axis=1)
    countif = (medal_rate.iloc[:-1, :] >= target_rate).sum(axis=0)
    medal_rate = pd.concat(
        [medal_rate, pd.DataFrame([countif], index=[(f"count_{target_rate}+", "")])],
        axis=0,
    )

    sheet_name = "MEDAL_RATE"
    rows, cols = medal_rate.shape
    try:
        worksheet = spreadsheet.worksheet(sheet_name)
        logger.info(f"âœ… ã‚·ãƒ¼ãƒˆã€Œ{sheet_name}ã€ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
    except gspread.exceptions.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(
            title=sheet_name, rows=str(rows + 5), cols=str(cols + 5)
        )
        logger.info(f"ğŸ†• ã‚·ãƒ¼ãƒˆã€Œ{sheet_name}ã€ã‚’æ–°è¦ä½œæˆã—ã¾ã—ãŸã€‚")

    sheet = spreadsheet.worksheet(sheet_name)
    sheet.clear()
    set_with_dataframe(sheet, medal_rate, include_index=True)
    sheet.update_cell(1, 1, today.strftime("%Y-%m-%d UPDATED"))
    logger.info(f"ğŸ’¾ medal_rate ã‚’ GSpread ã«æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")


def merge_all_model_date_to_gspread(df, spreadsheet, sheet_name):
    today = datetime.date.today()
    sheet = spreadsheet.worksheet(sheet_name)
    sheet.clear()
    set_with_dataframe(sheet, df, include_index=True)
    sheet.update_cell(1, 1, today.strftime("UPDATED: %Y-%m-%d"))
    logger.info(f"ğŸ’¾ ã‚·ãƒ¼ãƒˆæ›´æ–°å®Œäº†: {sheet_name}")


if __name__ == "__main__":

    # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ˆã‚Šãƒ›ãƒ¼ãƒ«åå–å¾—
    SEARCH_WORD = "EXA FIRST"
    # SEARCH_WORD = "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶å‚æˆ¸1000"
    # SEARCH_WORD = "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶ç‹­å±±åº—"

    SHEET_NAME_RANK = "RANKING"
    SHEET_NAME_COMPARE = "HISTORY"
    MODEL_LIST = [
        "ãƒã‚¤ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼V",
        "ã‚´ãƒ¼ã‚´ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼3",
        "ã‚¢ã‚¤ãƒ ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼EX-TP",
        "ãƒ•ã‚¡ãƒ³ã‚­ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼2",
        "ãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
        "ã‚¦ãƒ«ãƒˆãƒ©ãƒŸãƒ©ã‚¯ãƒ«ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
        "ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼ã‚¬ãƒ¼ãƒ«ã‚º",
        "ãƒãƒƒãƒ”ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼VIII",
    ]
    SPREADSHEET_ID = SPREADSHEET_IDS[SEARCH_WORD]
    if SEARCH_WORD not in SPREADSHEET_IDS:
        raise ValueError(f"{SEARCH_WORD} ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    query = """
    -- å‡ºç‰ãƒ‡ãƒ¼ã‚¿ã«ãƒ›ãƒ¼ãƒ«åã¨æ©Ÿç¨®åã‚’çµåˆã—ã¦å–å¾—
    SELECT
        r.*, 
        h.name AS hall_name,     -- ãƒ›ãƒ¼ãƒ«åã‚’è¿½åŠ 
        m.name AS model_name     -- æ©Ÿç¨®åã‚’è¿½åŠ 
    FROM results r
    JOIN halls h ON r.hall_id = h.hall_id  -- ãƒ›ãƒ¼ãƒ«ã¨çµåˆ
    JOIN models m ON r.model_id = m.model_id  -- æ©Ÿç¨®ã¨çµåˆ
    WHERE h.name = ?  -- æŒ‡å®šãƒ›ãƒ¼ãƒ«ã®ã¿
    AND m.name LIKE '%ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼%'  -- ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼ç³»æ©Ÿç¨®ã«é™å®š
    ORDER BY r.date DESC, r.unit_no ASC;
    """

    AREA_MAP_PATH = f"C:/python/dataOnline/anaslo_02/json/{SEARCH_WORD}_area_map.json"

    spreadsheet = connect_to_spreadsheet(SPREADSHEET_ID)
    df_from_db = search_hall_and_load_data(SEARCH_WORD, query)
    df = preprocess_result_df(df_from_db, AREA_MAP_PATH)

    # RANKING ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
    # medals_summary_to_gspread(
    #     df, MODEL_LIST, spreadsheet, get_medals_summary, sheet_name=SHEET_NAME_RANK
    # )

    # HISTORY ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
    merged_by_model = extract_merge_all_model_date(
        extract_and_merge_model_data, df, MODEL_LIST
    )
    merge_all_model_date_to_gspread(
        merged_by_model, spreadsheet, sheet_name=SHEET_NAME_COMPARE
    )

    # MEDAL_RATE ç”¨ã®ãƒ”ãƒœãƒƒãƒˆå‡¦ç†ãƒ»å‡ºåŠ›
    medal_rate_summary_to_gspread(df, SEARCH_WORD, spreadsheet)

    # df.to_csv("for_df_check.csv", index=False, encoding="utf-8-sig")
    # logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: for_df_check.csv")
    # output_path = f"merged_by_model.csv"
    # merged_by_model.to_csv(output_path, encoding="utf_8_sig")
    # logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {output_path}")
