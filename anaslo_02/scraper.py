from start_chrome_driver import start_google_chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import re
from logger_setup import setup_logger
from config import LOG_PATH


REMOVE_ADS_SCRIPT = """
var ads = document.querySelectorAll('[id^="google_ads"], [class*="ads"], [class*="sponsored"]');
ads.forEach(ad => ad.remove());
"""

logger = setup_logger("scraper", log_file=LOG_PATH)

def click_date_link(driver, DAYS_AGO, MAX_RETRIES=3):
    """
    æŒ‡å®šã—ãŸDAYS_AGOã®æ—¥ä»˜ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€ãƒšãƒ¼ã‚¸é·ç§»ã‚’ç¢ºèªã™ã‚‹
    :param driver: Selenium WebDriver
    :param DAYS_AGO: éå»ã®æ—¥æ•°ï¼ˆ1=æœ€æ–°ã®æ—¥ä»˜ï¼‰
    :param MAX_RETRIES: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
    :return: æˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯False
    """
    retries = 0
    current_url = driver.current_url

    try:
        wait = WebDriverWait(driver, 10)
        date_links = wait.until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".table-data-cell a"))
        )

        if DAYS_AGO > len(date_links):
            logger.error(f"ã‚¨ãƒ©ãƒ¼: DAYS_AGO={DAYS_AGO} ã«å¯¾å¿œã™ã‚‹ãƒªãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return False

        target_link = date_links[DAYS_AGO - 1]
        logger.info(f"ğŸ—“ Target Date: {target_link.text}")

        while retries < MAX_RETRIES:
            driver.execute_script(
                "arguments[0].scrollIntoView({behavior: 'auto', block: 'center'});",
                target_link,
            )
            time.sleep(2)  # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®å¾…æ©Ÿ
            wait.until(EC.element_to_be_clickable(target_link))
            time.sleep(2)

            target_link.click()

            wait.until(EC.url_changes(current_url))
            time.sleep(2)

            if "google_vignette" in driver.current_url:
                retries += 1
                logger.info(f"âš  Google Ads detected. Retrying... ({retries}/{MAX_RETRIES})")
                driver.back()  # åºƒå‘Šãƒšãƒ¼ã‚¸ã‹ã‚‰æˆ»ã‚‹
                time.sleep(2)
                continue  # ãƒªãƒˆãƒ©ã‚¤

            logger.info(f"âœ… Navigation successful: {driver.current_url}")
            date, hall_name = extract_date_hall(driver.current_url)
            return date, hall_name

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")

    logger.error("ã‚¯ãƒªãƒƒã‚¯å¤±æ•—: ãƒªãƒˆãƒ©ã‚¤å›æ•°è¶…é")
    return False  # å¤±æ•—


def extract_date_hall(url):
    """
    URL ã‹ã‚‰æ—¥ä»˜ (YYYY-MM-DD) ã¨ãƒ›ãƒ¼ãƒ«åã‚’æŠ½å‡º
    :param url: è§£æã™ã‚‹URL
    :return: (date, hall_name) ã®ã‚¿ãƒ—ãƒ«
    """
    pattern = r"https://ana-slo\.com/(\d{4})-(\d{1,2})-(\d{1,2})-(.+)-data/"
    match = re.search(pattern, url)

    if match:
        year, month, day, hall_name = match.groups()
        date = f"{year}-{int(month):02d}-{int(day):02d}"  # æœˆæ—¥ã‚’ã‚¼ãƒ­åŸ‹ã‚
        hall_name = hall_name.replace(
            "-", "_"
        ).upper()  # "-" ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®ãæ›ãˆã¦å¤§æ–‡å­—ã«
        logger.info(f"ğŸ—“ Date extracted from URL: {date}")
        logger.info(f"ğŸ¢ Hall extracted from URL: {hall_name}")
        return date, hall_name
    else:
        return None, None  # ãƒãƒƒãƒã—ãªã„å ´åˆ


def click_machine_by_name(driver):
    """
    æŒ‡å®šã—ãŸæ©Ÿç¨®åã‚’æ¤œç´¢ã—ã€è©²å½“ã™ã‚‹è¦ç´ ã‚’ã‚¯ãƒªãƒƒã‚¯
    :param driver: Selenium WebDriver
    :param search_machine: æ¤œç´¢ã™ã‚‹æ©Ÿç¨®å
    :return: ã‚¯ãƒªãƒƒã‚¯æˆåŠŸ: True / å¤±æ•—: False
    """
    try:
        title = driver.title
        date_pattern = r"\d{4}/\d{2}/\d{2}"
        date_match = re.search(date_pattern, title)
        date = date_match.group().replace("/", "-") if date_match else None
        hall_pattern = r"\d{4}/\d{2}/\d{2} (.+?) ãƒ‡ãƒ¼ã‚¿ã¾ã¨ã‚"
        hall_match = re.search(hall_pattern, title)
        hall_name = hall_match.group(1) if hall_match else None

        logger.info(f"ğŸ—“ Date confirmed on page: {date}")
        logger.info(f"ğŸ¢ Hall confirmed on page: {hall_name}")

        return date, hall_name

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return False


def extract_and_save_model_data(driver, prefecture, hall_name, date, csv_path):
    """
    æŒ‡å®šã—ãŸæ©Ÿç¨®ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€CSVã«ä¿å­˜ã™ã‚‹
    :param driver: Selenium WebDriver
    :param section_num: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå· (è©²å½“æ©Ÿç¨®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
    :param hall_name: ãƒ›ãƒ¼ãƒ«å
    :param date: æ—¥ä»˜ (yyyy-mm-dd)
    :return: æˆåŠŸæ™‚ True, å¤±æ•—æ™‚ False
    """
    try:
        # ã™ã¹ã¦ã®æ©Ÿç¨®ã‚’è¡¨ç¤º
        button = driver.find_element(By.ID, "all_data_btn")
        button.click()

        # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å–å¾—
        rows = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, f"#all_data_table_wrapper > div.dataTables_scroll tr")
            ))

        if not rows:
            logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            # return False

        logger.info(f"â¬ Starting to fetch {len(rows)} rows...")
        logger.info("ğŸ“¥ Downloading data...")

        # ãƒ˜ãƒƒãƒ€ãƒ¼å–å¾—
        header_cells = rows[0].find_elements(By.TAG_NAME, "th")
        columns = [cell.text for cell in header_cells]

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        data = []
        for row in rows[2:]:  # æœ€çµ‚è¡Œã‚’é™¤å¤–
            cells = row.find_elements(By.TAG_NAME, "td")
            data.append([cell.text for cell in cells])
        logger.info(f"âœ… Row {len(data)}: Data fetch completed")

        # DataFrameã‚’ä½œæˆã—CSVã«ä¿å­˜
        df = pd.DataFrame(data, columns=columns)
        rename_columns ={"æ©Ÿç¨®å": "model_name", "å°ç•ªå·": "unit_no", "Gæ•°": "game", "å·®æš": "medals"}
        df.rename(columns=rename_columns, inplace=True)
        cols_to_convert = ["unit_no", "game", "medals", "BB", "RB"]
        for col in cols_to_convert:
            if col in df.columns:
                df[col] = df[col].replace(",", "", regex=True).astype(int)
            else:
                df[col] = 0
                
        # åŒã˜æ©Ÿç¨®ã‚’æŒ‡ã—ã¦ã„ã‚‹ãŒè¡¨è¨˜ã‚†ã‚Œå¯¾ç­–
        alias_map = {
            "SãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼KK": "ãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
            "S ãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼ KK": "ãƒŸã‚¹ã‚¿ãƒ¼ã‚¸ãƒ£ã‚°ãƒ©ãƒ¼",
        }
        df["model_name"] = df["model_name"].replace(alias_map)
        
        csv_name = f"{csv_path}{prefecture}_{hall_name}_{date}.csv"
        df.to_csv(csv_name, index=False, encoding="utf-8-sig")

        logger.info(rows[1].text)

        logger.info(f"ğŸ’¾ Data saved: {csv_name}")
        return True

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return False
    
    
def scraper_for_data(driver, days_ago, REMOVE_ADS_SCRIPT, CSV_PATH, PREF, URL, MAX_RETRIES=5, SLEEP_TIME=10):
    
    driver.get(URL)
    driver.execute_script(REMOVE_ADS_SCRIPT)
    driver.execute_script("document.body.style.zoom='80%'")
        
    
    click_date_link(driver, days_ago, MAX_RETRIES=MAX_RETRIES)
    driver.execute_script(REMOVE_ADS_SCRIPT)
    driver.execute_script("document.body.style.zoom='80%'")
    
    waiting_time = 10
    logger.info(f"â³ Please complete the manual verification within {waiting_time} seconds")
    time.sleep(waiting_time)

    date, hall_name = click_machine_by_name(driver)
    extract_and_save_model_data(driver, PREF, hall_name, date, CSV_PATH)
        
    time.sleep(SLEEP_TIME)
    


if __name__ == "__main__":
    
    CSV_PATH = "anaslo_02/csv/"

    PREF = "æ±äº¬éƒ½"
    HALL_NAME = "exa-first"
    
    PREF = "åŸ¼ç‰çœŒ"
    HALL_NAME = "ç¬¬ä¸€ãƒ—ãƒ©ã‚¶ç‹­å±±åº—"
    
    URL = f"https://ana-slo.com/ãƒ›ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿/{PREF}/{HALL_NAME}-ãƒ‡ãƒ¼ã‚¿ä¸€è¦§/"

    DAYS_AGO = 1
    RERIOD = 15

    driver = start_google_chrome("https://www.google.com/")

    for days_ago in range(DAYS_AGO, DAYS_AGO+RERIOD):
        scraper_for_data(driver, days_ago, REMOVE_ADS_SCRIPT, CSV_PATH, PREF, URL)

    driver.close()